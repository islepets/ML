## Проблема переобучения и недообучения в алгоритмах ML. Определение, способы решения. Обучающие кривые
Переобучение - модель черезмерно подстраивается под обучающую выборку, включая шум и прочее, модель перестает обобщать данные.
Способы решения для переобучения: L1 и L2 регулеризации, раняя остановка модели.

Недообучение - модель слишком проста, не находит закономерности и показывает большую ошибку, что на обучающей, что на тестовой выборке.
Способы решения для недообучения: увеличить сложность модели, дополнительное обучение.

Обучающие кривые - график, который показывает зависимость производительности модели от данных.
## Методы оценки моделей машинного обучения (r2, ROC AUC, precision/recall/f1, accuracy)
R2 - коэфф. детерминации, показывает обобщающую способность модели.

ROC AUC - метрика, которая показывает, насколько модель хорошо различает классы, хорошо работает для оценки бинарных показателей.

Precision - показывает, какая доля предсказанных положительных примеров действительно является положительной.

Recall - показывает, какую долю реально положительно-предсказанных примеров удалось правильно идентефицировать.

f1 - гармонически среднее между precision и recall. 2 * (Precision*Recall)/(Precision+Recall)/

Accuracy - показывает долю правильных предсказаний среди всех предсказаний.
## Логистическая регрессия (+бинарная кроссэнтропия)

Логистическая регрессия — это статистический алгоритм машинного обучения, который используется для прогнозирования вероятности какого-то события. В основе алгоритма лежит логистическая функция — сигмоид. Она преобразует линейную комбинацию признаков в вероятности от 0 до 1, то есть решает задачу классификации. 

Бинарная кросс-энтропия — это функция логистической ошибки, которая используется в модели логистической регрессии. Она применяется вместо MSE, так как если использовать результат сигмоиды (нелинейной функции) в MSE, то на выходе получится невыпуклая функция, глобальный минимум которой сложно найти. 

## Методы оптимизации в моделях ML: GD, SGD, Adam

GD - градиентный спуск, градиент показывает возрастание функции, если двигаться в противоположном направлении достигнем минимума.

SGD - стохастический градиентный спуск, обновляет параметры на основе одного слкчайно выбранного примера из обучающего набора данных. Процесс становится болле шумным, но мы избегаем поиска локальных минимумов.

Adam - адаптирует скорость обучения для кажлого отдельного параметра.
## Активационные функции в нейронных сетях: sigmoid, softmax, ReLU, tanh, leaky ReLU. Линейные и нелинейные функции активации. Варианты использования.

sigmoid - преобразует входное значение в диапазон от 0 до 1. σ(x) = 1 / (1 + e^(-x))

softmax - преобразует вектор необработанных оценок в распределение вероятностей по нескольким классам. Функция помогает определить вероятности для каждого класса. Softmax преобразует результаты слоя в выходное распределение вероятностей.

ReLU - Выводит входные данные напрямую, если они положительные, и ноль в противном случае. В скрытых слоях нейронной сети помогает внести нелинейность, позволяющую модели изучать сложные закономерности в данных.

tanh или гиперболический тангенс - нелинейная функция, которая отображает входные данные в диапазон от -1 до 1.

leaky ReLU - вариант ReLU, который допускает небольшой, ненулевой градиент, когда входное значение отрицательно.

## Регуляризация L1, L2 в линейной, логистической регресии и нейронных сетях. Dropout

L1-регулеризация(Lasso) - добавляет к функции потерь штраф, который представляет из себя сумму абсолютных значений весов.

L2-регулеризация(Ridge) - использует сумму квадратов весов, сглаживая распределение параметров.

Dropout:
1) Во время обучения этот слой случайным образом устанавливает активации части нейронов в предыдущем слое в ноль. Это предотвращает чрезмерную зависимость нейронов друг от друга.
2) Во время фазы тестирования слой отключается, и используются все нейроны.

## Нормализация данных. Batch Normalisation.

Нормализация данных — это процесс приведения данных или активаций внутри нейросети к более однородному статистическому виду. 

Суть Batch Normalization — выравнивание статистики активаций внутри каждого мини-батча (набора примеров, обрабатываемых одновременно). Для каждого признака вычисляют среднее и дисперсию по всем элементам батча, после чего данные нормализуют, а затем масштабируют обучаемыми параметрами.